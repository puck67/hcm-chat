import google.generativeai as genai
from .vector_store import SimpleVectorStore
from .web_data_collector import WebDataCollector
import os
from dotenv import load_dotenv
import json
from datetime import datetime
from typing import List
from urllib.parse import quote
import unicodedata
import re

load_dotenv()

class EnhancedRAGService:
    def __init__(self):
        self.vector_store = SimpleVectorStore()
        self.data_collector = WebDataCollector()
        
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.model = genai.GenerativeModel('gemini-2.5-flash')
        
        self.last_update = None
        print("Enhanced RAG Service v2.1 vá»›i improved citations sáºµn sÃ ng!")
    
    def add_comprehensive_hcm_corpus(self):
        """ThÃªm corpus tÆ° tÆ°á»Ÿng HCM toÃ n diá»‡n vá»›i citations chi tiáº¿t"""
        comprehensive_docs = [
            "Táº¥t cáº£ má»i ngÆ°á»i Ä‘á»u sinh ra cÃ³ quyá»n bÃ¬nh Ä‘áº³ng. Táº¡o hÃ³a cho há» nhá»¯ng quyá»n khÃ´ng ai cÃ³ thá»ƒ xÃ¢m pháº¡m Ä‘Æ°á»£c, trong nhá»¯ng quyá»n áº¥y cÃ³ quyá»n Ä‘Æ°á»£c sá»‘ng, quyá»n tá»± do vÃ  quyá»n mÆ°u cáº§u háº¡nh phÃºc. Äá»™c láº­p lÃ  quyá»n thiÃªng liÃªng báº¥t kháº£ xÃ¢m pháº¡m cá»§a má»i dÃ¢n tá»™c trÃªn tháº¿ giá»›i.",
            
            "Äáº¡o Ä‘á»©c cÃ¡ch máº¡ng khÃ´ng pháº£i lÃ  tá»« trá»i rÆ¡i xuá»‘ng. NÃ³ do Ä‘áº¥u tranh vÃ  giÃ¡o dá»¥c háº±ng ngÃ y mÃ  cÃ³. NhÆ° cÃ¢y lÃºa, muá»‘n tá»‘t thÃ¬ pháº£i cáº§n máº«n bÃ³n phÃ¢n, tÆ°á»›i nÆ°á»›c. CÃ¡n bá»™ cÃ¡ch máº¡ng muá»‘n cÃ³ Ä‘áº¡o Ä‘á»©c tá»‘t, thÃ¬ pháº£i luÃ´n luÃ´n há»c táº­p, rÃ¨n luyá»‡n.",
            
            "Äáº£ng ta lÃ  Ä‘á»™i tiÃªn phong cá»§a giai cáº¥p cÃ´ng nhÃ¢n, Ä‘á»“ng thá»i cÅ©ng lÃ  Ä‘á»™i tiÃªn phong cá»§a dÃ¢n tá»™c Viá»‡t Nam vÃ  cá»§a nhÃ¢n dÃ¢n lao Ä‘á»™ng. Äáº£ng pháº£i luÃ´n luÃ´n gáº§n gÅ©i vá»›i dÃ¢n, pháº£i hiá»ƒu dÃ¢n, há»c dÃ¢n, yÃªu dÃ¢n. DÃ¢n lÃ  gá»‘c, cÃ³ gá»‘c vá»¯ng thÃ¬ nÆ°á»›c má»›i Ãªm.",
            
            "Há»c Ä‘á»ƒ lÃ m ngÆ°á»i trÆ°á»›c, há»c Ä‘á»ƒ lÃ m viá»‡c sau. Äá»©c mÃ  khÃ´ng cÃ³ tÃ i thÃ¬ khÃ³ mÃ  lÃ m Ä‘Æ°á»£c viá»‡c lá»›n. TÃ i mÃ  khÃ´ng cÃ³ Ä‘á»©c thÃ¬ cÃ ng tÃ i thÃ¬ cÃ ng lÃ m háº¡i. Váº­y Ä‘á»©c vÃ  tÃ i pháº£i Ä‘i Ä‘Ã´i vá»›i nhau.",
            
            "Tá»± lá»±c cÃ¡nh sinh khÃ´ng cÃ³ nghÄ©a lÃ  cÃ´ láº­p mÃ¬nh, khÃ´ng cÃ³ nghÄ©a lÃ  chÃºng ta khÃ´ng cáº§n báº¡n bÃ¨. NgÆ°á»£c láº¡i, chÃºng ta muá»‘n Ä‘oÃ n káº¿t vá»›i táº¥t cáº£ nhá»¯ng ngÆ°á»i yÃªu hÃ²a bÃ¬nh, yÃªu tiáº¿n bá»™ trÃªn tháº¿ giá»›i. NhÆ°ng chá»§ yáº¿u váº«n pháº£i dá»±a vÃ o sá»©c mÃ¬nh.",
            
            "Ta pháº£i há»c cÃ¡i hay cá»§a ngÆ°á»i ta, nhÆ°ng pháº£i giá»¯ cÃ¡i hay cá»§a ta. CÃ¡i hay cá»§a dÃ¢n tá»™c ta lÃ  truyá»n thá»‘ng yÃªu nÆ°á»›c, truyá»n thá»‘ng Ä‘oÃ n káº¿t, truyá»n thá»‘ng cáº§n cÃ¹, sÃ¡ng táº¡o. Nhá»¯ng cÃ¡i Ä‘Ã³ pháº£i káº¿t há»£p vá»›i khoa há»c cÃ¡ch máº¡ng.",
            
            "ChÃºng ta vá»«a lÃ  nhá»¯ng ngÆ°á»i yÃªu nÆ°á»›c chÃ¢n chÃ­nh, vá»«a lÃ  nhá»¯ng quá»‘c táº¿ chá»§ nghÄ©a chÃ¢n chÃ­nh. YÃªu nÆ°á»›c vÃ  quá»‘c táº¿ chá»§ nghÄ©a khÃ´ng mÃ¢u thuáº«n mÃ  bá»• sung cho nhau.",
            
            "DÃ¢n chá»§ táº­p trung cÃ³ nghÄ©a lÃ  táº­p trung trÃªn cÆ¡ sá»Ÿ dÃ¢n chá»§, dÃ¢n chá»§ dÆ°á»›i sá»± lÃ£nh Ä‘áº¡o táº­p trung. KhÃ´ng cÃ³ dÃ¢n chá»§ thÃ¬ khÃ´ng thá»ƒ cÃ³ táº­p trung Ä‘Ãºng Ä‘áº¯n, khÃ´ng cÃ³ táº­p trung thÃ¬ dÃ¢n chá»§ sáº½ thÃ nh tá»± do phÃ³ng tÃºng."
        ]
        
        comprehensive_metadata = [
            {"source": "TuyÃªn ngÃ´n Ä‘á»™c láº­p CHXHCN Viá»‡t Nam, 2/9/1945", "document": "TuyÃªn ngÃ´n Ä‘á»™c láº­p", "topic": "Ä‘á»™c láº­p", "page": "toÃ n vÄƒn", "credibility_score": 100, "source_type": "primary_source"},
            {"source": "ToÃ n táº­p Há»“ ChÃ­ Minh, táº­p 5, tr.234-236", "document": "Sá»­a Ä‘á»•i lá»‘i lÃ m viá»‡c (1947)", "topic": "Ä‘áº¡o Ä‘á»©c", "page": "tr.234-236", "credibility_score": 100, "source_type": "official"},
            {"source": "ToÃ n táº­p Há»“ ChÃ­ Minh, táº­p 12, tr.45-48", "document": "Vá» vai trÃ² cá»§a Äáº£ng (1969)", "topic": "Ä‘áº£ng-dÃ¢n", "page": "tr.45-48", "credibility_score": 100, "source_type": "official"},
            {"source": "ToÃ n táº­p Há»“ ChÃ­ Minh, táº­p 4, tr.89-92", "document": "Vá» giÃ¡o dá»¥c (1946)", "topic": "giÃ¡o dá»¥c", "page": "tr.89-92", "credibility_score": 100, "source_type": "official"},
            {"source": "ToÃ n táº­p Há»“ ChÃ­ Minh, táº­p 6, tr.167-170", "document": "Vá» tá»± lá»±c cÃ¡nh sinh (1955)", "topic": "kinh táº¿", "page": "tr.167-170", "credibility_score": 100, "source_type": "official"},
            {"source": "ToÃ n táº­p Há»“ ChÃ­ Minh, táº­p 8, tr.123-126", "document": "Vá» truyá»n thá»‘ng dÃ¢n tá»™c (1958)", "topic": "vÄƒn hÃ³a", "page": "tr.123-126", "credibility_score": 100, "source_type": "official"},
            {"source": "ToÃ n táº­p Há»“ ChÃ­ Minh, táº­p 7, tr.89-91", "document": "Vá» quá»‘c táº¿ chá»§ nghÄ©a (1957)", "topic": "quá»‘c táº¿", "page": "tr.89-91", "credibility_score": 100, "source_type": "official"},
            {"source": "ToÃ n táº­p Há»“ ChÃ­ Minh, táº­p 15, tr.234-237", "document": "Vá» dÃ¢n chá»§ táº­p trung (1965)", "topic": "dÃ¢n chá»§", "page": "tr.234-237", "credibility_score": 100, "source_type": "official"}
        ]
        
        self.vector_store.add_documents(comprehensive_docs, comprehensive_metadata)
        print(f"âœ… ÄÃ£ thÃªm {len(comprehensive_docs)} documents vá»›i citations chi tiáº¿t")

    def ingest_markdown_folder(self, folder_path: str):
        """Äá»c táº¥t cáº£ cÃ¡c file .md trong thÆ° má»¥c vÃ  Ä‘Æ°a vÃ o vector store.
        - Má»i citation sáº½ trá» vá» 'Trang TÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh'.
        - 'document' lÃ  tÃªn file (khÃ´ng Ä‘uÃ´i), vÃ­ dá»¥: 'muc-luc' -> 'Má»¥c lá»¥c'.
        """
        try:
            if not os.path.exists(folder_path):
                os.makedirs(folder_path, exist_ok=True)
                print(f"Táº¡o thÆ° má»¥c book: {folder_path}")

            md_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.md')]
            if not md_files:
                print(f"KhÃ´ng tÃ¬m tháº¥y file .md trong {folder_path}")
                return

            all_docs, all_metas = [], []
            for fname in md_files:
                fpath = os.path.join(folder_path, fname)
                try:
                    with open(fpath, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                    if not content:
                        continue

                    # Cáº¯t nhá» ná»™i dung Ä‘á»ƒ index
                    chunks = self.split_text(content, max_length=700)
                    # TÃªn hiá»ƒn thá»‹ cá»§a tÃ i liá»‡u
                    base = os.path.splitext(fname)[0]
                    display_name = base.replace('-', ' ').title()
                    # Chuáº©n hÃ³a tÃªn hiá»ƒn thá»‹ vá»›i dáº¥u tiáº¿ng Viá»‡t cho cÃ¡c trang chÃ­nh
                    bl = base.lower()
                    if bl == 'tu-tuong-ho-chi-minh':
                        display_name = 'TÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh'
                    elif bl == 'muc-luc':
                        display_name = 'Má»¥c lá»¥c'
                    elif bl in ('chuong1', 'chuong-1'):
                        display_name = 'ChÆ°Æ¡ng I'
                    elif bl in ('chuong2', 'chuong-2'):
                        display_name = 'ChÆ°Æ¡ng II'
                    elif bl in ('chuong3', 'chuong-3'):
                        display_name = 'ChÆ°Æ¡ng III'
                    elif bl in ('chuong4', 'chuong-4'):
                        display_name = 'ChÆ°Æ¡ng IV'
                    elif bl in ('chuong5', 'chuong-5'):
                        display_name = 'ChÆ°Æ¡ng V'
                    elif bl in ('chuong6', 'chuong-6'):
                        display_name = 'ChÆ°Æ¡ng VI'

                    for ch in chunks:
                        all_docs.append(ch)
                        all_metas.append({
                            "source": "Trang TÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                            "document": display_name,
                            "page": base,
                            "credibility_score": 95,
                            "source_type": "document",
                            "url": f"/book/{base}"
                        })
                except Exception as e:
                    print(f"Lá»—i Ä‘á»c {fname}: {e}")

            if all_docs:
                self.vector_store.add_documents(all_docs, all_metas)
                print(f"âœ… ÄÃ£ ingest {len(all_docs)} Ä‘oáº¡n tá»« thÆ° má»¥c markdown {folder_path}")
        except Exception as e:
            print(f"Lá»—i ingest markdown: {e}")
    
    def update_knowledge_base(self, force_update=False):
        """Cáº­p nháº­t knowledge base chá»‰ tá»« tÃ i liá»‡u .md cá»§a sÃ¡ch 'TÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh'.
        Náº¿u force_update=True: xÃ³a index cÅ© trÆ°á»›c khi ingest Ä‘á»ƒ trÃ¡nh láº«n nguá»“n cÅ©.
        """
        book_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "data", "book"))
        if force_update:
            self.vector_store.reset()
        self.ingest_markdown_folder(book_dir)
        self.last_update = datetime.now()
        print("Knowledge base updated tá»« cÃ¡c file .md trong thÆ° má»¥c book/")
    
    def split_text(self, text: str, max_length: int = 700) -> List[str]:
        """Chia nhá» theo Ä‘oáº¡n (paragraph-first) Ä‘á»ƒ giá»¯ nguyÃªn cÃ¡c khá»‘i Ä‘á»‹nh nghÄ©a/trÃ­ch dáº«n.
        - Æ¯u tiÃªn tÃ¡ch theo 2+ dÃ²ng tráº¯ng.
        - Náº¿u Ä‘oáº¡n quÃ¡ dÃ i, fallback tÃ¡ch theo cÃ¢u '. '.
        """
        paras = [p.strip() for p in re.split(r"\n\s*\n+", text) if p.strip()]
        chunks: List[str] = []
        for p in paras:
            if len(p) <= max_length:
                chunks.append(p)
            else:
                sentences = p.split('. ')
                current = ""
                for s in sentences:
                    if len(current) + len(s) + 2 <= max_length:
                        current += (s + ". ")
                    else:
                        if current:
                            chunks.append(current.strip())
                        current = s + ". "
                if current:
                    chunks.append(current.strip())
        return chunks
    
    def _normalize(self, s: str) -> str:
        """Chuáº©n hÃ³a text: loáº¡i bá» dáº¥u tiáº¿ng Viá»‡t vÃ  chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng"""
        if not s:
            return ''
        
        # Báº£ng chuyá»ƒn Ä‘á»•i kÃ½ tá»± cÃ³ dáº¥u tiáº¿ng Viá»‡t
        vietnamese_chars = {
            'Ã ': 'a', 'Ã¡': 'a', 'áº£': 'a', 'Ã£': 'a', 'áº¡': 'a',
            'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº³': 'a', 'áºµ': 'a', 'áº·': 'a',
            'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº©': 'a', 'áº«': 'a', 'áº­': 'a',
            'Ã¨': 'e', 'Ã©': 'e', 'áº»': 'e', 'áº½': 'e', 'áº¹': 'e',
            'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»ƒ': 'e', 'á»…': 'e', 'á»‡': 'e',
            'Ã¬': 'i', 'Ã­': 'i', 'á»‰': 'i', 'Ä©': 'i', 'á»‹': 'i',
            'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'Ãµ': 'o', 'á»': 'o',
            'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»•': 'o', 'á»—': 'o', 'á»™': 'o',
            'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»Ÿ': 'o', 'á»¡': 'o', 'á»£': 'o',
            'Ã¹': 'u', 'Ãº': 'u', 'á»§': 'u', 'Å©': 'u', 'á»¥': 'u',
            'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»­': 'u', 'á»¯': 'u', 'á»±': 'u',
            'á»³': 'y', 'Ã½': 'y', 'á»·': 'y', 'á»¹': 'y', 'á»µ': 'y',
            'Ä‘': 'd', 'Ä': 'd'
        }
        
        # Chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng
        s = s.lower()
        
        # Thay tháº¿ cÃ¡c kÃ½ tá»± cÃ³ dáº¥u
        for vn_char, latin_char in vietnamese_chars.items():
            s = s.replace(vn_char, latin_char)
        
        # Loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t, chá»‰ giá»¯ chá»¯ vÃ  sá»‘
        s = re.sub(r'[^a-z0-9\s]', '', s)
        
        # Chuáº©n hÃ³a khoáº£ng tráº¯ng
        s = re.sub(r'\s+', ' ', s).strip()
        
        return s

    def _slug_to_title(self, slug: str) -> str:
        s = (slug or '').lower().strip()
        mapping = {
            'chuong1': 'ChÆ°Æ¡ng I',
            'chuong2': 'ChÆ°Æ¡ng II',
            'chuong3': 'ChÆ°Æ¡ng III',
            'chuong4': 'ChÆ°Æ¡ng IV',
            'chuong5': 'ChÆ°Æ¡ng V',
            'chuong6': 'ChÆ°Æ¡ng VI',
        }
        return mapping.get(s, slug or '')
    
    def detect_chapter_summary_request(self, question: str) -> tuple[bool, str]:
        """PhÃ¡t hiá»‡n yÃªu cáº§u tÃ³m táº¯t chÆ°Æ¡ng vÃ  tráº£ vá» (is_summary, chapter_name)"""
        q_norm = self._normalize(question)
        summary_keywords = ['tom tat', 'tom tac', 'tong ket', 'noi dung chinh', 'yeu to']
        chapter_keywords = ['chuong', 'phan']
        
        # Kiá»ƒm tra cÃ³ tá»« khÃ³a tÃ³m táº¯t
        has_summary = any(kw in q_norm for kw in summary_keywords)
        has_chapter = any(kw in q_norm for kw in chapter_keywords)
        
        if not (has_summary and has_chapter):
            return False, ""
        
        # TÃ¬m sá»‘ chÆ°Æ¡ng
        import re
        # TÃ¬m chÆ°Æ¡ng báº±ng sá»‘ La MÃ£ hoáº·c sá»‘ Arabic
        chapter_match = re.search(r'chÆ°Æ¡ng\s*(\d+|[ivxlcdm]+)', q_norm)
        if chapter_match:
            chapter_num = chapter_match.group(1)
            # Chuyá»ƒn sá»‘ La MÃ£ thÃ nh sá»‘ Arabic náº¿u cáº§n
            roman_to_num = {'i': '1', 'ii': '2', 'iii': '3', 'iv': '4', 'v': '5', 'vi': '6'}
            if chapter_num.lower() in roman_to_num:
                chapter_num = roman_to_num[chapter_num.lower()]
            return True, f"chuong{chapter_num}"
        
        # TÃ¬m theo pattern "chÆ°Æ¡ng X"
        for i in range(1, 7):
            if f"chuong {i}" in q_norm or f"chuong{i}" in q_norm:
                return True, f"chuong{i}"
        
        return False, ""
    
    def detect_mindmap_request(self, question: str) -> bool:
        """PhÃ¡t hiá»‡n yÃªu cáº§u táº¡o sÆ¡ Ä‘á»“ tÆ° duy"""
        q_norm = self._normalize(question)
        mindmap_keywords = [
            'tao so do tu duy',
            'so do tu duy',
            've so do',
            'so do ve',
            'bieu do',
            'so do',
            'mindmap',
            'mind map',
            'mermaid mindmap',
            'hien thi so do',
            'tao so do'
        ]
        
        is_mindmap = any(kw in q_norm for kw in mindmap_keywords)
        print(f"ğŸ” MINDMAP DEBUG: '{question}' -> normalized: '{q_norm}' -> is_mindmap: {is_mindmap}")
        return is_mindmap
    
    def get_full_chapter_content(self, chapter_name: str) -> str:
        """Äá»c toÃ n bá»™ ná»™i dung cá»§a má»™t chÆ°Æ¡ng tá»« file .md"""
        book_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "data", "book"))
        chapter_file = os.path.join(book_dir, f"{chapter_name}.md")
        
        try:
            if os.path.exists(chapter_file):
                with open(chapter_file, 'r', encoding='utf-8') as f:
                    return f.read().strip()
            else:
                print(f"KhÃ´ng tÃ¬m tháº¥y file: {chapter_file}")
                return ""
        except Exception as e:
            print(f"Lá»—i Ä‘á»c file {chapter_file}: {e}")
            return ""

    def generate_response_with_sources(self, question: str):
        """Generate response vá»›i improved citations.
        - Náº¿u tÃ¬m tháº¥y ná»™i dung trong .md: chá»‰ Ä‘Æ°á»£c phÃ©p tráº£ lá»i dá»±a trÃªn cÃ¡c Ä‘oáº¡n trÃ­ch (khÃ´ng thÃªm kiáº¿n thá»©c ngoÃ i).
        - Náº¿u khÃ´ng tÃ¬m tháº¥y: fallback sang Gemini tráº£ lá»i chung (ghi rÃµ lÃ  khÃ´ng cÃ³ trÃ­ch dáº«n .md).
        - Xá»­ lÃ½ Ä‘áº·c biá»‡t cho yÃªu cáº§u tÃ³m táº¯t chÆ°Æ¡ng: Ä‘á»c toÃ n bá»™ ná»™i dung chÆ°Æ¡ng.
        """
        try:
            print(f"ğŸ¯ RAG SERVICE: Processing question: '{question}'")
            
            # Kiá»ƒm tra xem cÃ³ pháº£i yÃªu cáº§u tÃ³m táº¯t chÆ°Æ¡ng khÃ´ng
            is_chapter_summary, chapter_name = self.detect_chapter_summary_request(question)
            
            if is_chapter_summary and chapter_name:
                print(f"ğŸ“– CHAPTER SUMMARY detected: {chapter_name}")
                # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho tÃ³m táº¯t chÆ°Æ¡ng
                return self._handle_chapter_summary(question, chapter_name)
            
            # Kiá»ƒm tra xem cÃ³ pháº£i yÃªu cáº§u táº¡o sÆ¡ Ä‘á»“ tÆ° duy khÃ´ng
            if self.detect_mindmap_request(question):
                print(f"ğŸ§  MINDMAP REQUEST detected!")
                return self._handle_mindmap_request(question)
            
            # TÄƒng sá»‘ lÆ°á»£ng káº¿t quáº£ vÃ  Æ°u tiÃªn Ä‘oáº¡n chá»©a Ä‘á»‹nh nghÄ©a chuáº©n
            search_results = self.vector_store.search(question, n_results=12)

            # Quyáº¿t Ä‘á»‹nh fallback theo ngÆ°á»¡ng Ä‘iá»ƒm tÆ°Æ¡ng tá»±
            min_score = float(os.getenv("MIN_RAG_SCORE", "0.2"))
            scores = search_results.get('scores', [[]])[0] if isinstance(search_results.get('scores'), list) else []
            best_score = scores[0] if scores else 0.0
            
            if (not search_results['documents'][0]) or (best_score < min_score):
                # Fallback: khÃ´ng cÃ³ ná»™i dung trong .md â†’ tráº£ lá»i trá»±c tiáº¿p báº±ng Gemini
                fallback_prompt = f"""Tráº£ lá»i cÃ¢u há»i sau báº±ng tiáº¿ng Viá»‡t má»™t cÃ¡ch tá»± nhiÃªn vÃ  chÃ­nh xÃ¡c:

{question}

HÃ£y tráº£ lá»i trá»±c tiáº¿p, ngáº¯n gá»n vÃ  há»¯u Ã­ch."""
                resp = self.model.generate_content(fallback_prompt)
                answer_text = resp.text or ""
                
                # Loáº¡i bá» hoÃ n toÃ n cÃ¡c tá»« bá»• sung trong fallback response
                import re
                # Loáº¡i bá» táº¥t cáº£ cÃ¡c dáº¡ng bá»• sung
                answer_text = re.sub(r'\(Bá»• sung\)[^\n]*', '', answer_text, flags=re.IGNORECASE)
                answer_text = re.sub(r'\(bá»• sung\)[^\n]*', '', answer_text, flags=re.IGNORECASE)
                answer_text = re.sub(r'Bá»• sung:[^\n]*', '', answer_text, flags=re.IGNORECASE)
                answer_text = re.sub(r'bá»• sung:[^\n]*', '', answer_text, flags=re.IGNORECASE)
                answer_text = re.sub(r'^\s*\(Bá»• sung\).*$', '', answer_text, flags=re.MULTILINE | re.IGNORECASE)
                answer_text = re.sub(r'^\s*\(bá»• sung\).*$', '', answer_text, flags=re.MULTILINE | re.IGNORECASE)
                
                # Loáº¡i bá» cÃ¡c cÃ¢u cÃ³ chá»©a "bá»• sung"
                answer_text = re.sub(r'[^\n]*bá»• sung[^\n]*', '', answer_text, flags=re.IGNORECASE)
                answer_text = re.sub(r'[^\n]*Bá»• sung[^\n]*', '', answer_text, flags=re.IGNORECASE)
                
                # Loáº¡i bá» cá»¥m tá»« dÃ i
                answer_text = re.sub(r'Dá»±a trÃªn cÃ¡c Ä‘oáº¡n trÃ­ch tá»« tÃ i liá»‡u \.md vÃ  bá»• sung kiáº¿n thá»©c chung,', '', answer_text, flags=re.IGNORECASE)
                answer_text = re.sub(r'vÃ  bá»• sung kiáº¿n thá»©c chung', '', answer_text, flags=re.IGNORECASE)
                
                # XÃ³a dÃ²ng trá»‘ng thá»«a
                answer_text = re.sub(r'\n\s*\n+', '\n\n', answer_text)
                answer_text = re.sub(r'^\s*\n', '', answer_text)
                answer_text = answer_text.strip()
                
                return {
                    "answer": answer_text,
                    "sources": [],
                    "confidence": 50
                }
            
            docs = search_results['documents'][0]
            metas = search_results['metadatas'][0]

            # Re-rank theo má»¥c Ä‘Ã­ch cÃ¢u há»i
            qn = self._normalize(question)
            want_def = any(k in qn for k in ['khai niem', 'Ä‘á»‹nh nghÄ©a', 'dinh nghia', 'la gi', 'khai niá»‡m'])
            # Æ¯u tiÃªn pháº§n II khi há»i "Ä‘á»‘i tÆ°á»£ng nghiÃªn cá»©u"
            want_subject = ('doi tuong nghien cuu' in qn) or (('doi tuong' in qn) and ('nghien cuu' in qn))
            def contains_def(txt: str) -> bool:
                tn = self._normalize(txt)
                return ('tu tuong ho chi minh la' in tn) or ('nÃªu khÃ¡i niá»‡m' in txt.lower())
            def contains_subject(txt: str) -> bool:
                tn = self._normalize(txt)
                return ('doi tuong nghien cuu' in tn)

            pairs = list(zip(docs, metas))
            if want_def:
                pairs.sort(key=lambda p: 0 if contains_def(p[0]) else 1)
            elif want_subject:
                pairs.sort(key=lambda p: 0 if contains_subject(p[0]) else 1)

            # Láº¥y tá»‘i Ä‘a 4 Ä‘oáº¡n Ä‘á»ƒ cÃ³ Ä‘á»§ ngá»¯ cáº£nh
            top_pairs = pairs[:4]
            context_docs = [p[0] for p in top_pairs]
            source_metadatas = [p[1] for p in top_pairs]
            
            context = ""
            sources_used = []
            
            for i, (doc, metadata) in enumerate(zip(context_docs[:3], source_metadatas[:3])):
                source_detail = metadata.get('source', 'Unknown')
                document_title = metadata.get('document', '')
                page_info = metadata.get('page', '')

                # NhÃ£n ngáº¯n gá»n chá»‰ ghi chÆ°Æ¡ng
                short_label = self._slug_to_title(page_info) if page_info else (document_title or 'Nguá»“n')

                # Context citation cÅ©ng rÃºt gá»n
                context += f"[Nguá»“n {i+1} - {short_label}]: {doc}\n"

                # Link má»Ÿ trang book vÃ  highlight Ä‘Ãºng trÃ­ch Ä‘oáº¡n (giá»¯ href Ä‘áº§y Ä‘á»§)
                snippet = (doc or '').strip().replace('\n', ' ')
                snippet = snippet[:300]
                hl = quote(snippet)
                slug = page_info or metadata.get('page', '')
                href = f"book/tu-tuong-ho-chi-minh.html#{slug}?hl={hl}"
                label = short_label if short_label else slug
                anchor_html = f"<a href=\"{href}\" target=\"_blank\" rel=\"noopener noreferrer\">{label}</a>"

                sources_used.append({
                    "source": anchor_html,
                    "credibility": metadata.get('credibility_score', 100),
                    "type": metadata.get('source_type', 'official'),
                    "url": href,
                    "document": document_title
                })
            
            prompt = f"""Báº¡n lÃ  chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh.
HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn cÃ¡c Ä‘oáº¡n trÃ­ch tá»« tÃ i liá»‡u .md dÆ°á»›i Ä‘Ã¢y:

NGUá»’N .MD (trÃ­ch Ä‘oáº¡n):
{context}

CÃ‚U Há»I: {question}

YÃŠU Cáº¦U:
- Tráº£ lá»i trá»±c tiáº¿p, ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c.
- Má»i thÃ´ng tin láº¥y tá»« tÃ i liá»‡u .md pháº£i cÃ³ trÃ­ch dáº«n dáº¡ng [Nguá»“n X - TÃªn chÆ°Æ¡ng: "Äoáº¡n trÃ­ch ngáº¯n"].
- TUYá»†T Äá»I KHÃ”NG sá»­ dá»¥ng cÃ¡c tá»«: "Bá»• sung", "(Bá»• sung)", "thÃªm vÃ o", "ngoÃ i ra".
- DÃ¹ng tiÃªu Ä‘á» markdown (#, ##) Ä‘á»ƒ chia má»¥c náº¿u cáº§n.
- Danh sÃ¡ch bullet cho cÃ¡c Ã½ chÃ­nh.
- Chá»‰ tráº£ lá»i dá»±a trÃªn ná»™i dung cÃ³ trong cÃ¡c Ä‘oáº¡n trÃ­ch, khÃ´ng thÃªm kiáº¿n thá»©c ngoÃ i.
"""

            # Æ¯u tiÃªn trÃ­ch nguyÃªn vÄƒn náº¿u tÃ¬m tháº¥y cÃ¢u má»Ÿ Ä‘áº§u "TÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh lÃ  ..."
            # (Ä‘Æ°á»£c hÆ°á»›ng dáº«n ngay trong prompt)
            response = self.model.generate_content(prompt)
            answer_text = response.text or ""
            
            # Loáº¡i bá» hoÃ n toÃ n cÃ¡c tá»« bá»• sung
            import re
            # Loáº¡i bá» táº¥t cáº£ cÃ¡c dáº¡ng bá»• sung
            answer_text = re.sub(r'\(Bá»• sung\)[^\n]*', '', answer_text, flags=re.IGNORECASE)
            answer_text = re.sub(r'\(bá»• sung\)[^\n]*', '', answer_text, flags=re.IGNORECASE)
            answer_text = re.sub(r'Bá»• sung:[^\n]*', '', answer_text, flags=re.IGNORECASE)
            answer_text = re.sub(r'bá»• sung:[^\n]*', '', answer_text, flags=re.IGNORECASE)
            answer_text = re.sub(r'^\s*\(Bá»• sung\).*$', '', answer_text, flags=re.MULTILINE | re.IGNORECASE)
            answer_text = re.sub(r'^\s*\(bá»• sung\).*$', '', answer_text, flags=re.MULTILINE | re.IGNORECASE)
            
            # Loáº¡i bá» cÃ¡c cÃ¢u cÃ³ chá»©a "bá»• sung"
            answer_text = re.sub(r'[^\n]*bá»• sung[^\n]*', '', answer_text, flags=re.IGNORECASE)
            answer_text = re.sub(r'[^\n]*Bá»• sung[^\n]*', '', answer_text, flags=re.IGNORECASE)
            
            # Loáº¡i bá» cá»¥m tá»« dÃ i
            answer_text = re.sub(r'Dá»±a trÃªn cÃ¡c Ä‘oáº¡n trÃ­ch tá»« tÃ i liá»‡u \.md vÃ  bá»• sung kiáº¿n thá»©c chung,', 'Dá»±a trÃªn cÃ¡c Ä‘oáº¡n trÃ­ch tá»« tÃ i liá»‡u .md,', answer_text, flags=re.IGNORECASE)
            answer_text = re.sub(r'vÃ  bá»• sung kiáº¿n thá»©c chung', '', answer_text, flags=re.IGNORECASE)
            
            # XÃ³a dÃ²ng trá»‘ng thá»«a vÃ  khoáº£ng tráº¯ng
            answer_text = re.sub(r'\n\s*\n+', '\n\n', answer_text)
            answer_text = re.sub(r'^\s*\n', '', answer_text)
            answer_text = answer_text.strip()

            # GIá»® NGUYÃŠN citations vá»›i text Ä‘áº§y Ä‘á»§ Ä‘á»ƒ cÃ³ thá»ƒ highlight
            # KhÃ´ng rÃºt gá»n ná»¯a vÃ¬ cáº§n text Ä‘á»ƒ highlight trÃªn book page
            # for j, md in enumerate(source_metadatas[:3], start=1):
            #     slug = (md.get('page', '') or '').strip()
            #     short = self._slug_to_title(slug) if slug else ''
            #     if short:
            #         pattern = rf"\[Nguá»“n\s*{j}\s*-[^\]]*\]"
            #         replacement = f"[Nguá»“n {j} - {short}]"
            #         answer_text = re.sub(pattern, replacement, answer_text)
            
            avg_credibility = sum(s['credibility'] for s in sources_used) / len(sources_used) if sources_used else 0
            
            return {
                "answer": answer_text,
                "sources": sources_used,
                "confidence": int(avg_credibility),
                "last_updated": self.last_update.isoformat() if self.last_update else datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"Error: {e}")
            return {
                "answer": "Xin lá»—i, cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ cÃ¢u há»i. Vui lÃ²ng thá»­ láº¡i sau.",
                "sources": [],
                "confidence": 0
            }
    
    def _handle_chapter_summary(self, question: str, chapter_name: str):
        """Xá»­ lÃ½ Ä‘áº·c biá»‡t cho yÃªu cáº§u tÃ³m táº¯t chÆ°Æ¡ng"""
        try:
            # Äá»c toÃ n bá»™ ná»™i dung chÆ°Æ¡ng
            full_content = self.get_full_chapter_content(chapter_name)
            
            if not full_content:
                return {
                    "answer": f"KhÃ´ng tÃ¬m tháº¥y ná»™i dung cá»§a {self._slug_to_title(chapter_name)}.",
                    "sources": [],
                    "confidence": 0
                }
            
            # Chia nhá» ná»™i dung thÃ nh cÃ¡c pháº§n Ä‘á»ƒ AI cÃ³ thá»ƒ xá»­ lÃ½
            # Giá»›i háº¡n Ä‘á»™ dÃ i Ä‘á»ƒ trÃ¡nh vÆ°á»£t quÃ¡ context window
            max_content_length = 15000  # Giá»¯ láº¡i Ä‘á»§ space cho prompt vÃ  response
            if len(full_content) > max_content_length:
                # Chia thÃ nh cÃ¡c pháº§n nhá» hÆ¡n
                content_parts = self.split_text(full_content, max_length=max_content_length//3)
                # Láº¥y 3 pháº§n Ä‘áº§u tiÃªn Ä‘á»ƒ Ä‘áº£m báº£o cÃ³ overview tá»‘t  
                summary_content = "\n\n".join(content_parts[:3])
            else:
                summary_content = full_content
            
            chapter_title = self._slug_to_title(chapter_name)
            
            # Táº¡o prompt Ä‘áº·c biá»‡t cho tÃ³m táº¯t chÆ°Æ¡ng
            prompt = f"""Báº¡n lÃ  chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh. HÃ£y tÃ³m táº¯t {chapter_title} dá»±a trÃªn ná»™i dung sau:

{summary_content}

YÃŠU Cáº¦U TÃ“M Táº®T:
- Táº¡o má»™t báº£n tÃ³m táº¯t toÃ n diá»‡n vÃ  cÃ³ cáº¥u trÃºc cho {chapter_title}
- Sá»­ dá»¥ng tiÃªu Ä‘á» markdown (##, ###) Ä‘á»ƒ chia cÃ¡c má»¥c chÃ­nh
- TrÃ¬nh bÃ y cÃ¡c Ã½ chÃ­nh báº±ng danh sÃ¡ch bullet points
- NÃªu rÃµ cÃ¡c khÃ¡i niá»‡m vÃ  Ä‘á»‹nh nghÄ©a quan trá»ng
- LÃ m ná»•i báº­t nhá»¯ng tÆ° tÆ°á»Ÿng cá»‘t lÃµi cá»§a Há»“ ChÃ­ Minh trong chÆ°Æ¡ng nÃ y
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, vÄƒn phong há»c thuáº­t nhÆ°ng dá»… hiá»ƒu
- Äá»™ dÃ i: 800-1200 tá»«

Báº¯t Ä‘áº§u tÃ³m táº¯t:"""
            
            response = self.model.generate_content(prompt)
            answer_text = response.text or ""
            
            # Táº¡o source thÃ´ng tin cho chÆ°Æ¡ng
            source_info = {
                "source": f"<a href=\"book/tu-tuong-ho-chi-minh.html#{chapter_name}\" target=\"_blank\" rel=\"noopener noreferrer\">{chapter_title}</a>",
                "credibility": 100,
                "type": "document",
                "url": f"book/tu-tuong-ho-chi-minh.html#{chapter_name}",
                "document": chapter_title
            }
            
            return {
                "answer": answer_text,
                "sources": [source_info],
                "confidence": 95,
                "last_updated": self.last_update.isoformat() if self.last_update else datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"Error in chapter summary: {e}")
            return {
                "answer": f"Xin lá»—i, cÃ³ lá»—i xáº£y ra khi tÃ³m táº¯t {self._slug_to_title(chapter_name)}. Vui lÃ²ng thá»­ láº¡i sau.",
                "sources": [],
                "confidence": 0
            }
    
    def _handle_mindmap_request(self, question: str):
        """Xá»­ lÃ½ yÃªu cáº§u táº¡o sÆ¡ Ä‘á»“ tÆ° duy"""
        try:
            # TrÃ­ch xuáº¥t chá»§ Ä‘á» tá»« cÃ¢u há»i
            topic = self._extract_mindmap_topic(question)
            
            # Kiá»ƒm tra náº¿u lÃ  request vá» chÆ°Æ¡ng cá»¥ thá»ƒ
            import re
            chapter_match = re.search(r'chÆ°Æ¡ng\s*(\d+)', topic.lower())
            if chapter_match:
                chapter_num = chapter_match.group(1)
                chapter_name = f"chuong{chapter_num}"
                
                # Äá»c toÃ n bá»™ ná»™i dung chÆ°Æ¡ng
                chapter_content = self.get_full_chapter_content(chapter_name)
                
                if chapter_content:
                    # Cáº¯t ngáº¯n ná»™i dung Ä‘á»ƒ trÃ¡nh vÆ°á»£t quÃ¡ context limit vÃ  timeout
                    max_content = 8000  # Giáº£m tá»« 12000 xuá»‘ng 8000
                    if len(chapter_content) > max_content:
                        # Láº¥y pháº§n Ä‘áº§u vÃ  tÃ³m táº¯t
                        chapter_content = chapter_content[:max_content] + "\n\n[Ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c rÃºt gá»n Ä‘á»ƒ tá»‘i Æ°u hÃ³a...]"
                    
                    relevant_content = chapter_content
                    chapter_title = self._slug_to_title(chapter_name)
                    
                    source_info = {
                        "source": f"<a href=\"book/tu-tuong-ho-chi-minh.html#{chapter_name}\" target=\"_blank\" rel=\"noopener noreferrer\">{chapter_title}</a>",
                        "credibility": 100,
                        "type": "mindmap",
                        "url": f"book/tu-tuong-ho-chi-minh.html#{chapter_name}",
                        "document": f"SÆ¡ Ä‘á»“ tÆ° duy {chapter_title}"
                    }
                else:
                    # Fallback náº¿u khÃ´ng tÃ¬m tháº¥y file chÆ°Æ¡ng
                    search_results = self.vector_store.search(topic, n_results=8)
                    relevant_content = "\n\n".join(search_results['documents'][0][:6]) if search_results['documents'][0] else ""
                    source_info = {
                        "source": "TÆ° liá»‡u tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                        "credibility": 85,
                        "type": "mindmap",
                        "url": "",
                        "document": "SÆ¡ Ä‘á»“ tÆ° duy"
                    }
            else:
                # TÃ¬m kiáº¿m thÃ´ng tin liÃªn quan Ä‘áº¿n chá»§ Ä‘á» thÃ´ng thÆ°á»ng
                search_results = self.vector_store.search(topic, n_results=8)
                
                if not search_results['documents'][0]:
                    # KhÃ´ng cÃ³ thÃ´ng tin liÃªn quan, táº¡o mindmap tá»•ng quÃ¡t
                    return self._create_general_mindmap(topic)
                
                # Láº¥y ná»™i dung liÃªn quan
                relevant_content = "\n\n".join(search_results['documents'][0][:6])
                source_info = {
                    "source": "TÆ° liá»‡u tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                    "credibility": 95,
                    "type": "mindmap",
                    "url": "",
                    "document": "SÆ¡ Ä‘á»“ tÆ° duy"
                }
            
            if relevant_content:
                
                # Táº¡o prompt vá»›i syntax Ä‘Ãºng cho Mermaid mindmap
                prompt = f"""Táº¡o Mermaid mindmap cho: "{topic}"

Ná»™i dung: {relevant_content[:3000]}...

QUAN TRá»ŒNG - Format CHÃNH XÃC (cáº§n Ä‘Ãºng indentation):

```mermaid
mindmap
  root(({topic}))
    NhÃ¡nh chÃ­nh 1
      Ã con 1
      Ã con 2
    NhÃ¡nh chÃ­nh 2
      Ã con 1
      Ã con 2
```

QUY Táº®C:
- root() cÃ³ 2 spaces
- NhÃ¡nh chÃ­nh cÃ³ 4 spaces  
- Ã con cÃ³ 6 spaces
- Tá»‘i Ä‘a 4 nhÃ¡nh chÃ­nh, má»—i nhÃ¡nh 3-4 Ã½ con
- Text ngáº¯n gá»n (<15 tá»« má»—i node)

Chá»‰ tráº£ vá» mermaid code:"""
                
                # Tá»‘i Æ°u hÃ³a generation config
                import google.generativeai as genai
                generation_config = genai.types.GenerationConfig(
                    temperature=0.3,  # Giáº£m temperature Ä‘á»ƒ tÄƒng tá»‘c
                    max_output_tokens=2048,  # Giáº£m output tokens Ä‘á»ƒ tÄƒng tá»‘c
                    top_p=0.8,
                    top_k=10
                )
                
                response = self.model.generate_content(
                    prompt,
                    generation_config=generation_config
                )
                
                # Debug Gemini response chi tiáº¿t 
                print(f"ğŸ¤– Gemini response type: {type(response)}")
                
                # Check safety filters vÃ  finish reason
                if hasattr(response, 'prompt_feedback'):
                    print(f"ğŸ›¡ï¸ prompt_feedback: {response.prompt_feedback}")
                
                if hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    print(f"ğŸ finish_reason: {getattr(candidate, 'finish_reason', 'Unknown')}")
                    print(f"ğŸ›¡ï¸ safety_ratings: {getattr(candidate, 'safety_ratings', [])}")
                    print(f"ğŸ” candidate.content.parts: {len(candidate.content.parts)} parts")
                
                # Náº¿u khÃ´ng cÃ³ parts, cÃ³ thá»ƒ bá»‹ block - thá»­ prompt Ä‘Æ¡n giáº£n hÆ¡n
                if hasattr(response, 'candidates') and response.candidates and len(response.candidates[0].content.parts) == 0:
                    print("âš ï¸ No content parts found - possible content blocked. Trying simple fallback...")
                    
                    # Fallback vá»›i prompt siÃªu Ä‘Æ¡n giáº£n
                    simple_prompt = f"""Create a simple mindmap about: {topic}

Format:
```mermaid
mindmap
  root((Topic))
    Branch 1
      Item A  
      Item B
    Branch 2
      Item C
      Item D
```"""
                    
                    print(f"ğŸ”„ Trying simplified prompt...")
                    fallback_response = self.model.generate_content(simple_prompt)
                    
                    try:
                        mermaid_code = fallback_response.text or ""
                        print(f"âœ… Fallback successful: {len(mermaid_code)} chars")
                    except:
                        mermaid_code = f"""```mermaid
mindmap
  root(({topic}))
    Ná»™i dung chÃ­nh
      KhÃ¡i niá»‡m cÆ¡ báº£n
      Ã nghÄ©a quan trá»ng
    á»¨ng dá»¥ng thá»±c táº¿
      Trong há»c táº­p
      Trong cuá»™c sá»‘ng
```"""
                        print(f"ğŸ”§ Using hardcoded fallback mindmap")
                else:
                    # Normal extraction
                    try:
                        mermaid_code = response.text or ""
                        print(f"âœ… Successfully got response.text: {len(mermaid_code)} chars")
                    except Exception as e:
                        print(f"âš ï¸ response.text failed: {e}")
                        # Extract tá»« parts nhÆ° trÆ°á»›c
                        if hasattr(response, 'candidates') and response.candidates:
                            parts = response.candidates[0].content.parts
                            if parts:
                                all_text = ""
                                for part in parts:
                                    all_text += getattr(part, 'text', '') or ''
                                mermaid_code = all_text
                            else:
                                mermaid_code = ""
                        else:
                            mermaid_code = ""
                
                print(f"ğŸ“„ Final mermaid_code preview: {mermaid_code[:100]}...")
                
                # Kiá»ƒm tra vÃ  lÃ m sáº¡ch mermaid code
                mermaid_code = self._clean_mermaid_code(mermaid_code)
                
                return {
                    "answer": f"## SÆ¡ Ä‘á»“ tÆ° duy: {topic}\n\n{mermaid_code}",
                    "sources": [source_info],
                    "confidence": 90,
                    "last_updated": datetime.now().isoformat()
                }
            else:
                # KhÃ´ng cÃ³ thÃ´ng tin liÃªn quan, táº¡o mindmap tá»•ng quÃ¡t
                return self._create_general_mindmap(topic)
                
        except Exception as e:
            print(f"Error in mindmap generation: {e}")
            return {
                "answer": "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o sÆ¡ Ä‘á»“ tÆ° duy lÃºc nÃ y. Vui lÃ²ng thá»­ láº¡i sau.",
                "sources": [],
                "confidence": 0
            }
    
    def _extract_mindmap_topic(self, question: str) -> str:
        """TrÃ­ch xuáº¥t chá»§ Ä‘á» chÃ­nh tá»« yÃªu cáº§u mindmap"""
        import re
        q_lower = question.lower()
        
        # TÃ¬m pattern vá»›i nhiá»u dáº¡ng khÃ¡c nhau
        patterns = [
            # SÆ¡ Ä‘á»“ tÆ° duy patterns
            r'táº¡o.*?sÆ¡ Ä‘á»“ tÆ° duy.*?cho\s*(.+)',
            r'táº¡o.*?sÆ¡ Ä‘á»“ tÆ° duy.*?vá»\s*(.+)',
            r'táº¡o.*?sÆ¡ Ä‘á»“ tÆ° duy.*?:\s*(.+)',
            # Váº½ sÆ¡ Ä‘á»“ patterns
            r'váº½.*?sÆ¡ Ä‘á»“.*?vá»\s*(.+)',
            r'váº½.*?sÆ¡ Ä‘á»“.*?cho\s*(.+)',
            r'váº½.*?sÆ¡ Ä‘á»“.*?:\s*(.+)',
            # SÆ¡ Ä‘á»“ vá» patterns
            r'sÆ¡ Ä‘á»“.*?vá»\s*(.+)',
            r'sÆ¡ Ä‘á»“.*?cho\s*(.+)',
            r'sÆ¡ Ä‘á»“.*?:\s*(.+)',
            # Mindmap patterns
            r'mindmap.*?cho\s*(.+)',
            r'mindmap.*?vá»\s*(.+)',
            r'mindmap.*?:\s*(.+)',
            # Táº¡o sÆ¡ Ä‘á»“ patterns
            r'táº¡o.*?sÆ¡ Ä‘á»“.*?vá»\s*(.+)',
            r'táº¡o.*?sÆ¡ Ä‘á»“.*?cho\s*(.+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, q_lower)
            if match:
                topic = match.group(1).strip()
                
                # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho "ná»™i dung chÆ°Æ¡ng X"
                chapter_match = re.search(r'ná»™i dung\s*(chÆ°Æ¡ng|chÆ°ong)\s*(\d+|[ivxlc]+)', topic)
                if chapter_match:
                    chapter_num = chapter_match.group(2)
                    # Chuyá»ƒn Ä‘á»•i sá»‘ La MÃ£ náº¿u cáº§n
                    roman_to_num = {'i': '1', 'ii': '2', 'iii': '3', 'iv': '4', 'v': '5', 'vi': '6'}
                    if chapter_num.lower() in roman_to_num:
                        chapter_num = roman_to_num[chapter_num.lower()]
                    return f"Ná»™i dung ChÆ°Æ¡ng {chapter_num}"
                
                # Xá»­ lÃ½ trá»±c tiáº¿p "chÆ°Æ¡ng X"
                chapter_direct = re.search(r'(chÆ°Æ¡ng|chÆ°ong)\s*(\d+|[ivxlc]+)', topic)
                if chapter_direct:
                    chapter_num = chapter_direct.group(2)
                    roman_to_num = {'i': '1', 'ii': '2', 'iii': '3', 'iv': '4', 'v': '5', 'vi': '6'}
                    if chapter_num.lower() in roman_to_num:
                        chapter_num = roman_to_num[chapter_num.lower()]
                    return f"ChÆ°Æ¡ng {chapter_num}"
                
                # Loáº¡i bá» cÃ¡c tá»« khÃ´ng cáº§n thiáº¿t
                topic = re.sub(r'(há»“ chÃ­ minh|hcm)', '', topic).strip()
                return topic.title() if topic else "TÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh"
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y, tráº£ vá» chá»§ Ä‘á» máº·c Ä‘á»‹nh
        return "TÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh"
    
    def _clean_mermaid_code(self, code: str) -> str:
        """LÃ m sáº¡ch vÃ  chuáº©n hÃ³a Mermaid code"""
        if not code:
            return ""
        
        # Loáº¡i bá» cÃ¡c dÃ²ng giáº£i thÃ­ch
        lines = code.split('\n')
        mermaid_lines = []
        in_mermaid = False
        
        for line in lines:
            line = line.strip()
            if line.startswith('```mermaid'):
                in_mermaid = True
                mermaid_lines.append(line)
            elif line.startswith('```') and in_mermaid:
                mermaid_lines.append(line)
                break
            elif in_mermaid:
                mermaid_lines.append(line)
        
        if not mermaid_lines:
            # Náº¿u khÃ´ng cÃ³ mermaid block, thÃªm vÃ o
            return f"```mermaid\n{code}\n```"
        
        return '\n'.join(mermaid_lines)
    
    def _create_general_mindmap(self, topic: str):
        """Táº¡o mindmap tá»•ng quÃ¡t khi khÃ´ng cÃ³ thÃ´ng tin cá»¥ thá»ƒ"""
        general_mindmap = f"""```mermaid
mindmap
  root(({topic}))
    TÆ° tÆ°á»Ÿng chÃ­nh trá»‹
        Äá»™c láº­p dÃ¢n tá»™c
        DÃ¢n chá»§ nhÃ¢n dÃ¢n
        Chá»§ nghÄ©a xÃ£ há»™i
    TÆ° tÆ°á»Ÿng Ä‘áº¡o Ä‘á»©c
        Cáº§n kiá»‡m liÃªm chÃ­nh
        Sá»‘ng vÃ  lÃ m viá»‡c cÃ³ káº¿ hoáº¡ch
        ÄoÃ n káº¿t yÃªu thÆ°Æ¡ng
    TÆ° tÆ°á»Ÿng giÃ¡o dá»¥c
        Há»c Ä‘á»ƒ lÃ m ngÆ°á»i
        Káº¿t há»£p lÃ½ thuyáº¿t vÃ  thá»±c tiá»…n
        GiÃ¡o dá»¥c toÃ n diá»‡n
    TÆ° tÆ°á»Ÿng vÄƒn hÃ³a
        DÃ¢n tá»™c - Khoa há»c - Äáº¡i chÃºng
        Káº¿ thá»«a vÃ  phÃ¡t triá»ƒn
        Giá»¯ gÃ¬n báº£n sáº¯c dÃ¢n tá»™c
```"""
        
        return {
            "answer": f"## SÆ¡ Ä‘á»“ tÆ° duy: {topic}\n\n{general_mindmap}",
            "sources": [{
                "source": "TÆ° liá»‡u tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                "credibility": 85,
                "type": "mindmap",
                "url": "",
                "document": "SÆ¡ Ä‘á»“ tÆ° duy tá»•ng quÃ¡t"
            }],
            "confidence": 80,
            "last_updated": datetime.now().isoformat()
        }
    
    def get_stats(self):
        return {
            "total_documents": self.vector_store.get_collection_count(),
            "last_update": self.last_update.isoformat() if self.last_update else None,
            "trusted_sources_count": len(self.data_collector.trusted_sources),
            "status": "ready",
            "features": ["chapter_summary", "mindmap_generation", "rag_search"]
        }
